{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "94757cc6-a4d0-4829-8d16-d1052ed665d4",
      "metadata": {
        "id": "94757cc6-a4d0-4829-8d16-d1052ed665d4"
      },
      "source": [
        "# Chustomer Churn Prediction\n",
        "## Project Overview\n",
        "A Data from the Telco DomainDue to tough competition the customers tend to swtich between the telecommunication service providersE.gan Airtel customer might transition to Jio services and vice versaThis behaviour from the customers is known as churn.\n",
        "\n",
        "## Objective\n",
        "To be able to predict if a customer would churn or notTake the Next Best Action to prevent churn.\n",
        "\n",
        "## Stages to be convered during the solution\n",
        "- `Data Merging and Wrangling:` Combining multiple data sources and cleaning the data\n",
        "- `Exploratory Data Analysis:` Understanding the relationship between features and with target\n",
        "- `Data Preprocessing:` Data Encoding, Missing Value Treatment, Outlier Treatment, Feature Scaling\n",
        "- `Model Building:` Train ML Model using the pre-processed data\n",
        "- `Evaluation:` Assess the Model's performace\n",
        "\n",
        "By the end of this project, you will have a complete workflow for predicting churn and/or creating classification models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "236c2aca-9e82-46dc-b557-6cf825123c7f",
      "metadata": {
        "id": "236c2aca-9e82-46dc-b557-6cf825123c7f"
      },
      "source": [
        "## Domain Backgroud (Telecom Churn Stroy)\n",
        "\n",
        "I’m working as a data analyst at a telecommunications company, which I refer to as TeleComCoTeleComCo provides phone and internet services to a wide range of customers, and, as with most telecoms, customer churn—when customers stop using our service—is a key concernHigh churn rates lead to lost revenue and may signal customer dissatisfaction.\n",
        "\n",
        "Recently, in the last quarter, we noticed a rise in customers leaving for competitorsIn response, our management tasked my team with investigating the reasons behind this churn and developing a model to predict which customers are most at risk of leavingThe purpose is to identify these customers in advance and proactively offer them incentives to stay.\n",
        "\n",
        "For this project, I’ve been given a dataset detailing account information for both past and current customers, along with data indicating whether or not each customer eventually churnedMy primary responsibilities include:\n",
        "- Analyzing this dataset to detect patterns and factors associated with customer churn.\n",
        "- Building a predictive model (specifically using logistic regression) to estimate churn risk for individual customers.\n",
        "- Through this data exploration, I expect to identify patterns such as:\n",
        "- Customers with longer tenures are generally less likely to churn, while newer customers may be at greater risk.\n",
        "- Those with certain types of plans or higher monthly charges might be more inclined to leave, possibly due to the cost factor.\n",
        "- Demographic details could influence churn—for instance, senior citizens may use our services differently or have specific needs.\n",
        "\n",
        "Customer preferences, like opting for paperless billing or bundling phone and internet, might also relate to their likelihood of churning.\n",
        "By thoroughly investigating these factors and building the predictive model, we aim to help TeleComCo understand why customers leave and reduce future churn through timely interventions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f77035-7e8d-4c41-937e-1a25d3d734e0",
      "metadata": {
        "id": "93f77035-7e8d-4c41-937e-1a25d3d734e0"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "The dataset consists of customer records, each with a variety of features describing the customer and their service usage. Below is an overview of each column in the data:\n",
        "- `customer_id:` A unique identifier for each customer (e.g., a UUID). This is just an ID and not useful for prediction.\n",
        "- `customer_email:` The email address-of the customer. This is an identifier as well and not directly useful for the model.\n",
        "- `age:` The age of the customer (in years). This could be related to churn if different age groups have different service preferences.\n",
        "- `senior_citizen:` Whether the customer is a senior citizen or not (boolean: true/false). Typically, this might be derived from age (e.g., age > 65).\n",
        "- `partner:` Whether the customer has a partner or not (boolean). This indicates if the customer is married or in a long-term partnershipIn telecom, having a partner\n",
        "might mean family plans or shared services.\n",
        "- `dependents:` Whether the customer has dependents (children or other dependents) or not (boolean) Customers with dependents might have different usage (e.g.,family plans).\n",
        "- `tenure_months:` The number of months the customer has been with the company. Higher tenure might indicate loyalty; low tenure customers are newer and might be more likely to churn if they haven't established loyalty.\n",
        "- `phone_service:` Whether the customer has phone service with the company (boolean). Some customers might only have internet service; this feature tells if they also subscribed to phone.\n",
        "- `paperless_billing:` Whether the customer has opted for paperless billing (boolean). This could be a proxy for tech-savvy behavior or convenience preference.\n",
        "- `monthly_charges:` The amount `$` charged to the customer every month. This is like their monthly bill. Customers with higher bills might churn due to cost, or those with very low bills might churn if they are not using many services.\n",
        "- `total_charges:` The total amount `$` the customer has been charged since joining (this is roughly monthly_charges * tenure, plus any extras). This can indicate the overall value of the customer; low total charges might mean the customer is relatively new or has a low-cost plan.\n",
        "- `churn:` The target variable - whether the customer has churned (true = yes, the customer left; false = no, the customer is still with the company). This is what we want to\n",
        "predict.\n",
        "- `last_interaction_date:` The date of the last interaction with the customer (could be the last service use or last customer support call, etc.). This might give insight into how recently the customer was active. Customers with very old last interactions might have silently churned.\n",
        "- `region:` The geographic region or state where the customer resides (e.g., Ohio, California, etc.). Different regions might have different market conditions or competitor\n",
        "presence, possibly affecting churn.\n",
        "- `signup_date:` The date when the customer originally signed up for service. (Note: This column is present in one of the source files. When we merge data, some records might not have a signup_date if it wasn't recorded for them.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c853bb-a2a7-4752-95c1-96ac8fd98115",
      "metadata": {
        "id": "09c853bb-a2a7-4752-95c1-96ac8fd98115"
      },
      "source": [
        "## Potential Questions and Considerations:\n",
        "\n",
        "Based on the above features, here are some questions that might arise and that we will explore in this project:\n",
        "- Do older customers or senior citizens tend to churn more or less than younger customers?\n",
        "- Does having a partner or dependents influence churn? (For example, do single customers churn more often than those with family plans?)\n",
        "- How does tenure relate to churn? Are newer customers more likely to leave compared to long-term customers?\n",
        "- What about monthly charges? Are customers with high monthly charges more likely to churn (perhaps due to higher cost), or could it be that those with low charges churn because they might not be fully utilizing the service?\n",
        "- Are there any regional trends in churn? (We might check if certain regions have higher churn rates.)\n",
        "- How do features like phone service or paperless billing correlate with churn? (e.g., maybe paperless billing users are more engaged or maybe less personal interaction leads to higher churn?)\n",
        "- Are there outliers or unusual values in charges or tenure that need special attention?\n",
        "  \n",
        "`Try to answer these questions step-by-step in the analysis below.`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "423ef82c-5e34-4180-8ab5-2e9f64ef72fc",
      "metadata": {
        "id": "423ef82c-5e34-4180-8ab5-2e9f64ef72fc"
      },
      "source": [
        "### Task 0: Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3e5ccbbb-7aea-48e4-96ef-3aae4f3cc880",
      "metadata": {
        "id": "3e5ccbbb-7aea-48e4-96ef-3aae4f3cc880"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "# to tell python to show the pyplot in the outplut seciton of the cell\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc8eb83-15eb-47fc-b11f-da7d7c5dcd9e",
      "metadata": {
        "id": "7cc8eb83-15eb-47fc-b11f-da7d7c5dcd9e"
      },
      "source": [
        "### Task 1: Combine the two datasets\n",
        "The customer data is provided in two CSV files (say, Customer Churn_data.cv and Customer Churn data_2.cv). Load both files and combine them into a single pandas\n",
        "DataFrame for analysis. The two files have the same columns (one file may have an extra column signup_date). Ensure that after merging, all columns are aligned correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "09c6ae3b-06df-4318-813e-99fdb0d449ab",
      "metadata": {
        "id": "09c6ae3b-06df-4318-813e-99fdb0d449ab",
        "outputId": "90d422c9-4f7e-4cb5-d46e-45a14daaafe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Customer_Churn_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3142546024.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customer_Churn_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Customer_Churn_data.csv'"
          ]
        }
      ],
      "source": [
        "file1 = pd.read_csv(\"Customer_Churn_data.csv\")\n",
        "print(file1.shape)\n",
        "file1.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4745f66e-ac76-4836-bcdd-592def3f05fb",
      "metadata": {
        "id": "4745f66e-ac76-4836-bcdd-592def3f05fb"
      },
      "outputs": [],
      "source": [
        "file2 = pd.read_csv(\"Customer_Churn_data_2.csv\")\n",
        "print(file2.shape)\n",
        "file2.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "351ecaa1-aee3-4710-85c6-2eafd8d63eb9",
      "metadata": {
        "id": "351ecaa1-aee3-4710-85c6-2eafd8d63eb9"
      },
      "source": [
        "### Task 2: View the first few rows of the combined data\n",
        "\n",
        "After merging, use the DataFrame's head© method to display the first 5 rows of the combined dataset. This will help verify that the data from both files has been concatenated\n",
        "correctly and that columns are as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdf7d6e-91ac-4715-ad1f-f3b48aa87406",
      "metadata": {
        "id": "7fdf7d6e-91ac-4715-ad1f-f3b48aa87406"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([file1, file2], ignore_index=True)\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86c0f592-504d-41c0-ae7f-9a63d668407f",
      "metadata": {
        "id": "86c0f592-504d-41c0-ae7f-9a63d668407f"
      },
      "source": [
        "### Task 3: Understand the dataset dimensions and dtypes\n",
        "Determine the size of the combined dataset. Find out how many rows and columns are present. This can be done using the DataFrame's .info0 method. This will show the data type of each column and whether there are any missing values (non-null counts) in each column. Verfy that numeric columns are correctly recognized (e.g. age.\n",
        "tenure months should be int or float, charges should be float, churn and otner booleans might appear as bool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af229325-f4fa-4f64-9200-27d51deffc92",
      "metadata": {
        "id": "af229325-f4fa-4f64-9200-27d51deffc92"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3bc861a-f40e-438d-addd-9c9f583aaa06",
      "metadata": {
        "id": "e3bc861a-f40e-438d-addd-9c9f583aaa06"
      },
      "source": [
        "`Verify if the data has been loaded perfectly`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7c0638-aacd-4d49-8659-3be3c12c107c",
      "metadata": {
        "id": "ad7c0638-aacd-4d49-8659-3be3c12c107c"
      },
      "outputs": [],
      "source": [
        "# last_interaction_date --> convert to datetime\n",
        "data['last_interaction_date'] = pd.to_datetime(data['last_interaction_date'])\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0cb16d6-7d6b-446b-af4a-4f0d6de93700",
      "metadata": {
        "id": "c0cb16d6-7d6b-446b-af4a-4f0d6de93700"
      },
      "outputs": [],
      "source": [
        "# signup_date --> convert into correct datatype (datetime)\n",
        "data['signup_date'] = pd.to_datetime(data['signup_date']) #infer_datetime_formate=True --> to handle mixed formats in a single col\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162b88fb-b377-4b8f-90b6-c585fb4429a6",
      "metadata": {
        "id": "162b88fb-b377-4b8f-90b6-c585fb4429a6"
      },
      "source": [
        "### Task 4: Generate summary statistics\n",
        "Use the .describe() method on the DataFrame to get summary statistics for the numeric columns (count, mean, std, min, quartiles, max). This will give an overview of the\n",
        "distributions (e.g., average age, average tenure, min/max charges, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a7bbf1-6c47-4a14-9bf3-ccda452756ec",
      "metadata": {
        "id": "22a7bbf1-6c47-4a14-9bf3-ccda452756ec"
      },
      "outputs": [],
      "source": [
        "data.describe(include=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d50870e-06fc-418a-8c68-a74225fce9c3",
      "metadata": {
        "id": "4d50870e-06fc-418a-8c68-a74225fce9c3"
      },
      "outputs": [],
      "source": [
        "data.describe(include=int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d84490-17e7-44c5-be53-1f015ebba7fd",
      "metadata": {
        "id": "48d84490-17e7-44c5-be53-1f015ebba7fd"
      },
      "outputs": [],
      "source": [
        "data.describe(include=float)\n",
        "# tenure*monthly charges = total_charges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "814a8a07-03aa-400f-9df5-347e18bc6b33",
      "metadata": {
        "id": "814a8a07-03aa-400f-9df5-347e18bc6b33"
      },
      "outputs": [],
      "source": [
        "data.describe(include=bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f478ef-e80b-43bb-a50f-97f57087a763",
      "metadata": {
        "id": "03f478ef-e80b-43bb-a50f-97f57087a763"
      },
      "outputs": [],
      "source": [
        "data.describe(include=['datetimetz'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f298daa7-55ca-43e1-8611-da4a1d3a4a42",
      "metadata": {
        "id": "f298daa7-55ca-43e1-8611-da4a1d3a4a42"
      },
      "source": [
        "### Task 5: Check for duplicate entries\n",
        "Ensure there are no duplicate customer records in the data. For instance, verify if customer_id is unique across the combined dataset. You can use pandas functions like\n",
        ".duplicated() on the customer_id column to check for any duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917e0c9d-70a0-4b69-9c78-80a3884a06e7",
      "metadata": {
        "id": "917e0c9d-70a0-4b69-9c78-80a3884a06e7"
      },
      "outputs": [],
      "source": [
        "data['customer_id'].duplicated().sum() #number of duplicated rows in the given col/df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5b9cf86-917b-460d-b455-99f5a4cabb3d",
      "metadata": {
        "id": "b5b9cf86-917b-460d-b455-99f5a4cabb3d"
      },
      "outputs": [],
      "source": [
        "data.duplicated().sum() # number of duplicated rows in the given df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dd3da38-e11e-4a75-80d0-2e6aad7bc0f7",
      "metadata": {
        "id": "7dd3da38-e11e-4a75-80d0-2e6aad7bc0f7"
      },
      "outputs": [],
      "source": [
        "data.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56069fab-9026-47f7-8a8e-36a9f74cda7a",
      "metadata": {
        "id": "56069fab-9026-47f7-8a8e-36a9f74cda7a"
      },
      "source": [
        "### Task 6: Identify missing values\n",
        "Identify if there are any missing values in the dataset and in which columns. Use methods like .isnull().sum() to get the count of null or NaN values per column. This will highlight columns that need attention (e.g., we expect many missing in signup_date if one file lacked it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af54fdef-99d6-4ce0-835a-bfc666a0388d",
      "metadata": {
        "id": "af54fdef-99d6-4ce0-835a-bfc666a0388d"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19df9439-09ed-4d42-bc9f-140b0197575e",
      "metadata": {
        "id": "19df9439-09ed-4d42-bc9f-140b0197575e"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(data.isnull())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8dc7ff3-3a4f-475d-8ce5-babad6981cb5",
      "metadata": {
        "id": "a8dc7ff3-3a4f-475d-8ce5-babad6981cb5"
      },
      "source": [
        "### Task 7: Analyze the pattern of missing data\n",
        "- Examine the missing data pattern and determine the likely mechanism: - Are the missing values MCAR (Missing Completely at Random) - i.e., no identifiable pattern, just\n",
        "random? - Or MAR (Missing At Random) - i.e., the missingness is related to some other observed data? - Or MNAR (Missing Not at Random) - i.e., the missingness has a\n",
        "pattern related to the unobserved value itself or is systematically absent for a particular subset?\n",
        "\n",
        "- Question: Based on the columns with missing data, what type of missingness do you suspect? For example, if signup_date is missing for all customers from one file, that's a systematic pattern (likely MNAR or a data collection issue). Document your reasoning. --> It is not missing at random, it is missing on purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e075bfb2-cff4-4739-800a-5f1c5b9cfc9c",
      "metadata": {
        "id": "e075bfb2-cff4-4739-800a-5f1c5b9cfc9c"
      },
      "source": [
        "### Task 8: Handle missing values\n",
        "- Decide on a strategy to handle the missing data identified above. For instance: - If a column has too many missing values (or is not crucial), you might choose to drop that column. - If only a few records have missing values, you might choose to fill (impute) them with an appropriate value (mean, median, mode, or a special indicator).\n",
        "- Apply the chosen strategy. For example, if signup_date is missing for a large portion and not critical to the analysis, you might drop the signup_date column to simplify the\n",
        "dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60c5940e-c83e-4216-8a08-799c984e47fd",
      "metadata": {
        "id": "60c5940e-c83e-4216-8a08-799c984e47fd"
      },
      "source": [
        "### Task 9: Drop unneeded columns\n",
        "- There are some columns that are not useful for predicting churn and could be removed to simplify the analysis. Typically, identifier columns like customer_id and customer_email do not have predictive value. Also, if we have decided not to use certain columns (like dates or any others) for modeling, we can drop them as well to avoid clutter.\n",
        "- Remove the following: - customer_id and customer_email (identifiers) - last_interaction_date (a date field that we will not use in the model for now, to keep things simple) -\n",
        "signup_date (if you did not drop it already in the missing data step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2906ecc-f2d6-42b9-99cc-d99a08a0868a",
      "metadata": {
        "id": "e2906ecc-f2d6-42b9-99cc-d99a08a0868a"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f68c938-1823-4959-ba75-eefd65ddfe5e",
      "metadata": {
        "id": "4f68c938-1823-4959-ba75-eefd65ddfe5e"
      },
      "outputs": [],
      "source": [
        "data.set_index('customer_id', inplace=True) # we can choose to make cols with all unique values as row index for better reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e61692f-7306-442e-a6fe-1e7fd31b61e2",
      "metadata": {
        "id": "4e61692f-7306-442e-a6fe-1e7fd31b61e2"
      },
      "outputs": [],
      "source": [
        "data.sample(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394d3d57-488c-41b5-b5e6-ed9a69454887",
      "metadata": {
        "id": "394d3d57-488c-41b5-b5e6-ed9a69454887"
      },
      "outputs": [],
      "source": [
        "# since email is  a PII (Personal Information Identifier) it should never be used as a machine learning feature\n",
        "# since there is no logic to replace/fill the missing values for signup_date, we would drop the column\n",
        "print(data.shape)\n",
        "data.drop(['customer_email', 'signup_date'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dbce622-40e9-4a73-a8f8-90961c527340",
      "metadata": {
        "id": "3dbce622-40e9-4a73-a8f8-90961c527340"
      },
      "outputs": [],
      "source": [
        "print(data.shape)\n",
        "data.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d795067-70f4-4224-92cf-ba3df6ce9b94",
      "metadata": {
        "id": "7d795067-70f4-4224-92cf-ba3df6ce9b94"
      },
      "outputs": [],
      "source": [
        "# RFM modeling --> Recency (How recent), Frequency (How frequent), Monitory (How much pay)\n",
        "# here in the data: last_interaction_date is Recency, tenure_months is Frequency and total_charges is Monitory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba79da65-d552-44fd-bee6-7ffa3a694d23",
      "metadata": {
        "id": "ba79da65-d552-44fd-bee6-7ffa3a694d23"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "Now that the data is clean and prepared, let's perform some exploratory analysis to understand the data better and to gather insights about what factors might affect churn.\n",
        "We will look at the distribution of variables and relationships between features and the churn outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41beff1d-0cd5-4766-99f2-e393293668fe",
      "metadata": {
        "id": "41beff1d-0cd5-4766-99f2-e393293668fe"
      },
      "source": [
        "### Task 10: Examine the distribution of the target variable (Churn)\n",
        "\n",
        "Let's see how many customers in our dataset churned vs. stayed. Plot a count of churned vs non-churned customers. This can be done using a bar plot (or simply checking the value counts). This will tell us the balance of our classes (churn vs no churn)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cfe53a4-4c63-49bc-9e01-09a44eca30af",
      "metadata": {
        "id": "4cfe53a4-4c63-49bc-9e01-09a44eca30af"
      },
      "outputs": [],
      "source": [
        "data.churn.value_counts().plot(kind='bar') # We have a balanced data set with 50:50 rows for both calsses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df0281d-6af3-455c-bdcf-529665189202",
      "metadata": {
        "id": "4df0281d-6af3-455c-bdcf-529665189202"
      },
      "source": [
        "### Task 11: Distribution of customer ages\n",
        "Plot a histogram of the age of customers. This will show the distribution of customer ages. Are most customers in a certain age range? This might help identify if our customer\n",
        "base is younger or older on average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ce9a6f2-93cf-44ec-91ad-76fb6d5fe15a",
      "metadata": {
        "id": "4ce9a6f2-93cf-44ec-91ad-76fb6d5fe15a"
      },
      "outputs": [],
      "source": [
        "data.age.hist(bins=20) # The age distribution seems to be uniformly distributed\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9324719-ce85-4ebb-be17-8100362e8116",
      "metadata": {
        "id": "e9324719-ce85-4ebb-be17-8100362e8116"
      },
      "outputs": [],
      "source": [
        "data.boxplot('age')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3020adc1-6039-4e8f-8d22-5b9c977724de",
      "metadata": {
        "id": "3020adc1-6039-4e8f-8d22-5b9c977724de"
      },
      "outputs": [],
      "source": [
        "data.age.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "440710a5-121c-4a24-a6e5-7fa3a59c0c66",
      "metadata": {
        "id": "440710a5-121c-4a24-a6e5-7fa3a59c0c66"
      },
      "source": [
        "### Task 12: Distribution of customer tenure\n",
        "Plot a histogram of the tenure_months to see how long customers tend to stay with the company. Is there a large number of new customers (low tenure) in the data? Do we see many customers at the maximum tenure (72 months, if that's the max)? Understanding tenure distribution will help in analyzing churn by tenure later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de69c96a-c488-4aef-9262-a9c1e94e102b",
      "metadata": {
        "id": "de69c96a-c488-4aef-9262-a9c1e94e102b"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data.tenure_months)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197335d0-2982-49ed-8d37-101da6776c2d",
      "metadata": {
        "id": "197335d0-2982-49ed-8d37-101da6776c2d"
      },
      "outputs": [],
      "source": [
        "data.boxplot('tenure_months')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5dcdce4-560c-41b9-a672-0fda985eebb7",
      "metadata": {
        "id": "e5dcdce4-560c-41b9-a672-0fda985eebb7"
      },
      "outputs": [],
      "source": [
        "data.tenure_months.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fad4a9-1136-4f0c-9342-a884f96a6ef2",
      "metadata": {
        "id": "b3fad4a9-1136-4f0c-9342-a884f96a6ef2"
      },
      "outputs": [],
      "source": [
        "data.tenure_months.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2773204-accb-48d5-bd7c-3901353c7302",
      "metadata": {
        "id": "e2773204-accb-48d5-bd7c-3901353c7302"
      },
      "source": [
        "- `Analysis: we can see customers who have spent time over the full range. Uniform distribution`\n",
        "- `incase of uneven dist we can transform the variable into categories like new, <3M, <6M, <1Y, <2Y..`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a001c3d5-c3b8-4325-973f-329faf542b67",
      "metadata": {
        "id": "a001c3d5-c3b8-4325-973f-329faf542b67"
      },
      "source": [
        "### Task 13: Distribution of monthly charges\n",
        "Plot a histogram of the monthly_charges. This shows the distribution of monthly billing amounts. We can see the range of charges and if it's skewed (e.g., many customers at lower tiers vs higher tiers). Sometimes, very high or very low charges could influence churn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cfaf4bb-3170-45bd-bc3b-6d87b7a68457",
      "metadata": {
        "id": "1cfaf4bb-3170-45bd-bc3b-6d87b7a68457"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data.monthly_charges, bins=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a2ae41-7eac-404e-8c7b-3b6131a0e347",
      "metadata": {
        "id": "25a2ae41-7eac-404e-8c7b-3b6131a0e347"
      },
      "source": [
        "`Analysis: comparively the high paying customers are less in number`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1bf08ea-8383-446a-90b6-9763f18bceae",
      "metadata": {
        "id": "a1bf08ea-8383-446a-90b6-9763f18bceae"
      },
      "outputs": [],
      "source": [
        "data.monthly_charges.plot(kind='kde')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e87ac49-6662-4036-8806-6b3164d9bf10",
      "metadata": {
        "id": "8e87ac49-6662-4036-8806-6b3164d9bf10"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(data.monthly_charges)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa167acf-e5d0-493c-80a6-c4e542b99680",
      "metadata": {
        "id": "aa167acf-e5d0-493c-80a6-c4e542b99680"
      },
      "outputs": [],
      "source": [
        "data.monthly_charges.describe()\n",
        "# for floating variables value counts is not of much use for analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09d25ca-907d-4390-aaa6-17ced2becd83",
      "metadata": {
        "id": "a09d25ca-907d-4390-aaa6-17ced2becd83"
      },
      "source": [
        "### Task 14: Churn rate by senior citizen status\n",
        "- Question: Are senior citizens more likely to churn compared to non-senior customers? Calculate the churn rate for senior citizens vs non-senior citizens. Churn rate can be defined as the percentage of customers in that group who have churned.\n",
        "- You can do this by grouping the data by senior_citizen and calculating the mean of the churn column (if churn is encoded as 0/1, the mean gives the proportion that churned).\n",
        "Alternatively, use value_counts of churn within each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b5d764e-6f5c-487f-ad6e-bfcf09c48820",
      "metadata": {
        "id": "4b5d764e-6f5c-487f-ad6e-bfcf09c48820"
      },
      "outputs": [],
      "source": [
        "data.groupby('senior_citizen')['churn'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07bf8f4f-9a25-4b21-b83e-0203f169a832",
      "metadata": {
        "id": "07bf8f4f-9a25-4b21-b83e-0203f169a832"
      },
      "source": [
        "### Task 15: Churn rate by partner status\n",
        "Question: Does having a partner influence churn? Compute the churn rate for customers with a partner vs without a partner. Similar to above, group by partner and find the proportion that churned in each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8808fd8c-c2f7-4681-b422-1977532536cc",
      "metadata": {
        "id": "8808fd8c-c2f7-4681-b422-1977532536cc"
      },
      "outputs": [],
      "source": [
        "data.groupby('partner')['churn'].mean().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ebf9f4f-4097-49ea-8823-90dc137a2e5f",
      "metadata": {
        "id": "9ebf9f4f-4097-49ea-8823-90dc137a2e5f"
      },
      "source": [
        "### Task 16: Average tenure of churned vs non-churned customers\n",
        "Question: Do customers who churn tend to have shorter tenures? Calculate the average tenure (in months) for churned customers vs customers who stayed. This can be done\n",
        "by grouping by churn status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7462bde-f753-4491-a678-a32b47a280fa",
      "metadata": {
        "id": "c7462bde-f753-4491-a678-a32b47a280fa"
      },
      "outputs": [],
      "source": [
        "data.groupby('churn')['tenure_months'].mean().plot(kind='pie')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e88ca1cd-bd69-424b-9dc9-4a6c3edd0c54",
      "metadata": {
        "id": "e88ca1cd-bd69-424b-9dc9-4a6c3edd0c54"
      },
      "source": [
        "### Task 17: Average monthly charges of churned vs non-churned customers\n",
        "Question: Do customers who churn pay more per month? Find the average monthly_charges for churned vs non-churned groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0573c5-3de9-4149-8637-694247cae927",
      "metadata": {
        "id": "9d0573c5-3de9-4149-8637-694247cae927"
      },
      "outputs": [],
      "source": [
        "data.groupby('churn')['monthly_charges'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "501e40ff-c743-4254-919d-c1d4a785b36b",
      "metadata": {
        "id": "501e40ff-c743-4254-919d-c1d4a785b36b"
      },
      "source": [
        "### Task 18: Average total charges of churned vs non-churned customers\n",
        "Question: How do the total charges differ between churned and retained customers? Calculate the average total_charges for churned vs non-churned customers. (Since\n",
        "total_charges is a function of monthly charges and tenure, this will reflect both how long and how much a churned customer contributed versus a stayed customer.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95a25e5-7600-4edf-ad78-46537f5108f1",
      "metadata": {
        "id": "b95a25e5-7600-4edf-ad78-46537f5108f1"
      },
      "outputs": [],
      "source": [
        "data.groupby('churn')['total_charges'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee730247-540c-4e89-aace-098c8da38cfc",
      "metadata": {
        "id": "ee730247-540c-4e89-aace-098c8da38cfc"
      },
      "source": [
        "### Task 19: Correlation analysis\n",
        "Calculate the correlation matrix for the numeric features (and the churn indicator, encoded as 0/1). This will show how strongly features are linearly related to each other and to churn. In particular, look at correlations involving churn. Are any features strongly positively or negatively correlated with churn? Also note if any pair of features are highly correlated with each other (for instance, tenure and total_charges might be strongly correlated since longer tenure usually means more total charges)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca1cfe69-a711-4baa-9ab2-e9ce17ff5dd1",
      "metadata": {
        "id": "ca1cfe69-a711-4baa-9ab2-e9ce17ff5dd1"
      },
      "outputs": [],
      "source": [
        "corr=data.corr(numeric_only=True)\n",
        "corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49dde800-fc58-4731-9108-deb684872892",
      "metadata": {
        "id": "49dde800-fc58-4731-9108-deb684872892"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "sns.heatmap(corr, annot=True, cmap='Greens', center=0, fmt=\".3f\")\n",
        "# Analysis: Sicne none of the feature are highly correlated to any other feature, we do not need to drop any features\n",
        "# If x features a highly correlated, then keep 1 of them and drop X-1, e.g. if 7 features are highly correlated then keep ANY 1 and drop the rest."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41ab1bd0-3fd4-45d7-a02e-4cb0674c0c19",
      "metadata": {
        "id": "41ab1bd0-3fd4-45d7-a02e-4cb0674c0c19"
      },
      "source": [
        "## Outlier Detection and Treatment\n",
        "- Outliers are extreme values that deviate significantly from the rest of the data. They can affect our model, especially logistic regression which could be influenced by very large or small values. We will detect outliers in key numeric columns and decide on how to handle them.\n",
        "\n",
        "- We'll use two common methods: - Interquartile Range (IQR) method: We consider points as outliers if they fall below Q1 - 1.5/QR or above Q3 + 1.5IQR for a given feature. - Z-\n",
        "score method: We calculate the z-score (standard score) for each data point in a feature. A common rule is to treat points with |z| > 3 as potential outliers (3 standard\n",
        "deviations away from the mean).\n",
        "\n",
        "- We will apply these methods to the monthly_charges (as an example numeric feature, since charges could have outliers)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "240db848-1ca1-46b6-90d2-4c71b0aecbfa",
      "metadata": {
        "id": "240db848-1ca1-46b6-90d2-4c71b0aecbfa"
      },
      "source": [
        "### Task 20: Detect outliers in monthly_charges using the IQR method\n",
        "Calculate the first quartile (Q1) and third quartile (Q3) of monthly_charges, then compute the IQR (Q3 - Q1). Determine the IQR bounds:\n",
        "- Lower bound = Q1 - 1.5 * IQR\n",
        "- Upper bound = Q3 + 1.5 * IQR\n",
        "\n",
        "Find which data points in monthly_charges lie outside these bounds. How many outliers do you detect using this rule?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f3b1585-6498-429a-963d-7b25319e957a",
      "metadata": {
        "id": "2f3b1585-6498-429a-963d-7b25319e957a"
      },
      "outputs": [],
      "source": [
        "data.monthly_charges.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a3456c-552c-437f-ad67-902787878b6d",
      "metadata": {
        "id": "b2a3456c-552c-437f-ad67-902787878b6d"
      },
      "outputs": [],
      "source": [
        "Q1 = data.monthly_charges.quantile(0.25)\n",
        "Q3 = data.monthly_charges.quantile(0.75)\n",
        "\n",
        "IQR = Q3-Q1\n",
        "print(f\"Q1: {round(Q1,2)}\\nQ3: {round(Q3,2)}\\nIQR: {round(IQR,2)}\".format(\".2f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1acf2c52-8e0f-48d1-a760-26734e4e3f17",
      "metadata": {
        "id": "1acf2c52-8e0f-48d1-a760-26734e4e3f17"
      },
      "outputs": [],
      "source": [
        "lower_bound = Q1-1.5*IQR\n",
        "upper_bound = Q3+1.5*IQR\n",
        "print(f\"Lower Bound: {round(lower_bound,2)}\\nUpper Bound: {round(upper_bound,2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d0bd5c-ef23-4666-8255-1ad2f2cfc055",
      "metadata": {
        "id": "d4d0bd5c-ef23-4666-8255-1ad2f2cfc055"
      },
      "outputs": [],
      "source": [
        "ul_outliers = data[data['monthly_charges'] > upper_bound].shape[0]\n",
        "ll_outliers = data[data['monthly_charges']< lower_bound].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c35f0c3-8e0a-4b8a-b79e-eaa4438100de",
      "metadata": {
        "id": "4c35f0c3-8e0a-4b8a-b79e-eaa4438100de"
      },
      "outputs": [],
      "source": [
        "print(ul_outliers, ll_outliers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e19d873a-a853-4393-a97f-d6599ec9a830",
      "metadata": {
        "id": "e19d873a-a853-4393-a97f-d6599ec9a830"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(data.monthly_charges,color='Green')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e0e3e5a-b180-4247-a447-6a147fa836ef",
      "metadata": {
        "id": "0e0e3e5a-b180-4247-a447-6a147fa836ef"
      },
      "source": [
        "### Task 21: Detect outliers in monthly_charges using the Z-method (This method is used when you have more than 20% as outliers uisng the IQR method)\n",
        "Calculate the Mean and Standard Deviation of monthly_charges. Determine the Z-Methods bounds:\n",
        "- Lower bound = MEAN - 3 * STD_DEV\n",
        "- Upper bound = MEAN + 3 * STD_DEV\n",
        "\n",
        "Find which data points in monthly_charges lie outside these bounds. How many outliers do you detect using this rule?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6c0bd0f-11a0-4356-8515-517fc24d8d66",
      "metadata": {
        "id": "e6c0bd0f-11a0-4356-8515-517fc24d8d66"
      },
      "outputs": [],
      "source": [
        "mean = np.mean(data.monthly_charges)\n",
        "std = np.std(data.monthly_charges)\n",
        "\n",
        "LL = mean - 3 * std\n",
        "UL = mean + 3 * std\n",
        "\n",
        "ul_outliers = data[data['monthly_charges'] > UL].shape[0]\n",
        "ll_outliers = data[data['monthly_charges']< LL].shape[0]\n",
        "\n",
        "print(f\"Mean: {round(mean,2)} | Std Dev: {round(std,2)} | UL: {round(UL,2)} | LL: {round(LL,2)} | ul_outlier: {round(ll_outliers,2)} | ll_outliers: {round(ll_outliers,2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7423f132-300b-4834-90a9-a88a6cae1ec1",
      "metadata": {
        "id": "7423f132-300b-4834-90a9-a88a6cae1ec1"
      },
      "source": [
        "### Task 22: We can now, run the same analysis for other numeric(int and float) columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e0cf59-b37a-41ee-8832-7927b36a977f",
      "metadata": {
        "id": "39e0cf59-b37a-41ee-8832-7927b36a977f"
      },
      "source": [
        "`Outlier Treatment`\n",
        "\n",
        "Decide how to handle any outliers found in monthly_charges (and any other numeric columns if you checked them). Common strategies include: - Removing the outlier rows entirely. - Capping the outliers (e.g., set values above the upper bound to the upper bound, and below the lower bound to the lower bound). - Keeping them if they are legitimate values and not overly influential.\n",
        "\n",
        "For this analysis, if outliers exist and are very few, you might choose to remove those records for simplicity. Alternatively, if they are not extreme or numerous, you might leave them in but be aware of them.\n",
        "\n",
        "Implement the chosen outlier treatment for monthly_charges. (If no outliers were detected by either method, you can state that no action is needed or just skip removal.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91dc4046-b30f-412d-b7f9-5a02b0e34179",
      "metadata": {
        "id": "91dc4046-b30f-412d-b7f9-5a02b0e34179"
      },
      "source": [
        "## Data Preprocessing (Encoding, Splitting, Scaling)\n",
        "Before we can feed the data into a logistic regression model, we need to prepare the features: - Convert categorical and boolean features into numeric form (encoding). - Split the data into training and test sets. - Scale/normalize features if needed, so that no single feature dominates due to scale differences (this can help the model converge faster and improve performance)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fb17843-b3cb-4678-adcd-5f1dd27500a2",
      "metadata": {
        "id": "6fb17843-b3cb-4678-adcd-5f1dd27500a2"
      },
      "source": [
        "### Task 23: Encode the target variable churn as numeric\n",
        "\n",
        "The churn column is currently in boolean (true/false) form (or Yes/No). Convert it to a numeric binary format, e.g., 1 for \"Yes/True\" (customer churned) and 0 for \"No/False\" (customer stayed). This will be our target y for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdf0cb7-c759-428b-8e4e-0bd3faa1e1f2",
      "metadata": {
        "id": "8bdf0cb7-c759-428b-8e4e-0bd3faa1e1f2"
      },
      "outputs": [],
      "source": [
        "data.churn = data.churn.astype(int)\n",
        "data.churn.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e65718-393b-4aeb-8cda-66c19540760f",
      "metadata": {
        "id": "41e65718-393b-4aeb-8cda-66c19540760f"
      },
      "source": [
        "### Task 24: Convert other boolean columns to 0/1\n",
        "Similarly, convert all other boolean columns (senior_citizen, partner, dependents, phone_service, paperless_billing) into 0/1 numeric values (if they are not already numeric). This ensures that all features are in numeric form for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a76db7c2-98c2-4ed8-8076-5a763f87cf50",
      "metadata": {
        "id": "a76db7c2-98c2-4ed8-8076-5a763f87cf50"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c096ce46-1447-4fa3-bdd6-8c9cc8bb2cbe",
      "metadata": {
        "id": "c096ce46-1447-4fa3-bdd6-8c9cc8bb2cbe"
      },
      "outputs": [],
      "source": [
        "bool_cols = ['senior_citizen', 'partner', 'dependents', 'phone_service', 'paperless_billing']\n",
        "data[bool_cols] = data[bool_cols].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ccba7e-378e-4736-8095-9ef314b34987",
      "metadata": {
        "scrolled": true,
        "id": "96ccba7e-378e-4736-8095-9ef314b34987"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c286633c-bc34-4683-bfc5-6243e8c2192c",
      "metadata": {
        "id": "c286633c-bc34-4683-bfc5-6243e8c2192c"
      },
      "source": [
        "### Task 25: Extract Date Features like dayofweek, Month, Date, Year, Hour, Mins etc. and drop the timestamp col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73761d8-69ad-46a3-ba96-7b354d9d2572",
      "metadata": {
        "id": "d73761d8-69ad-46a3-ba96-7b354d9d2572"
      },
      "outputs": [],
      "source": [
        "data['day_of_week'] = data['last_interaction_date'].dt.weekday\n",
        "data['year'] = data['last_interaction_date'].dt.year\n",
        "data['month'] = data['last_interaction_date'].dt.month\n",
        "data['date'] = data['last_interaction_date'].dt.day\n",
        "data['hour'] = data['last_interaction_date'].dt.hour\n",
        "data['minute'] = data['last_interaction_date'].dt.minute\n",
        "data['second'] = data['last_interaction_date'].dt.second\n",
        "data['week_of_year'] = data['last_interaction_date'].dt.isocalendar().week\n",
        "data['quarter'] = data['last_interaction_date'].dt.quarter\n",
        "data.drop('last_interaction_date', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da561c4-b25b-4f4b-bd1e-387577adec4d",
      "metadata": {
        "id": "1da561c4-b25b-4f4b-bd1e-387577adec4d"
      },
      "outputs": [],
      "source": [
        "data.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6ccefe-2b40-4740-9d9f-7b8076c05922",
      "metadata": {
        "id": "1d6ccefe-2b40-4740-9d9f-7b8076c05922"
      },
      "source": [
        "### Task 26: One-hot encode the region column\n",
        "The region column is categorical with many possible values (states or regions). We need to convert it into a numeric form. Use one-hot encoding to create dummy variables for each unique region. For example, region \"Ohio\" becomes a binary column that is 1 for Ohio residents and 0 otherwise, and so on for each region.\n",
        "\n",
        "You can use pandas get_dummies function to do this. Be careful to avoid the dummy variable trap (when one dummy column is redundant because it can be inferred from\n",
        "others). You can set drop_first=True to drop one of the region dummy columns, or handle it manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27184e69-9434-4677-8ca7-76608207bb9e",
      "metadata": {
        "id": "27184e69-9434-4677-8ca7-76608207bb9e"
      },
      "outputs": [],
      "source": [
        "region_dummies = pd.get_dummies(data['region'], drop_first=True, dtype='int')\n",
        "\n",
        "data = pd.concat([data, region_dummies], axis=1)\n",
        "data.drop('region', axis=1, inplace=True)\n",
        "\n",
        "data.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2358d9b7-41ec-44ba-96f0-396e7b24d0c2",
      "metadata": {
        "id": "2358d9b7-41ec-44ba-96f0-396e7b24d0c2"
      },
      "source": [
        "### Task 27: Separate features and target variable\n",
        "Now that the data is preprocessed, split the DataFrame into features (X) and target (y). - y should be the churn column (the 0/1 labels we want to predict). - X should be all the remaining columns that will serve as inputs to the model.\n",
        "\n",
        "Make sure that X does not include the target itself or any columns we decided to drop (like IDs, emails, etc., which we already removed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f64062d6-8328-4f4e-a725-be1efa3dcf8c",
      "metadata": {
        "id": "f64062d6-8328-4f4e-a725-be1efa3dcf8c"
      },
      "outputs": [],
      "source": [
        "X = data.drop(columns='churn')\n",
        "y = data.churn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d988eb6-193f-400e-bbc6-3089e379f092",
      "metadata": {
        "id": "3d988eb6-193f-400e-bbc6-3089e379f092"
      },
      "source": [
        "### Task 28: Split data into training and testing sets\n",
        "Use sklearn.model_selection.train_test_split to split the dataset into training and test sets. Typically, we might use 70% of the data for training and 30% for testing (or 80/20,etc.). Set a random_state for reproducibility.\n",
        "\n",
        "The result should be X_train, X test, y_train, y_test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daf5b34a-64d2-497a-beed-2525f738dd62",
      "metadata": {
        "id": "daf5b34a-64d2-497a-beed-2525f738dd62"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) #try stratify=y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51abde1c-50dc-4cd1-9cfe-d0c38c8a7bab",
      "metadata": {
        "id": "51abde1c-50dc-4cd1-9cfe-d0c38c8a7bab"
      },
      "source": [
        "### Task 29: Feature scaling (Standardization)\n",
        "For logistic regression, it is often beneficial to scale the features so they are on comparable scales (although logistic regression can still work without scaling, scaling can improve convergence and performance, especially if regularization is used).\n",
        "\n",
        "Use a StandardScaler (from sklearn.preprocessing) to standardize the numeric features in X. Important: Fit the scaler on the training data only, then use it to transform both the training and testing feature data. This prevents information from the test set leaking into the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e05d9c0b-c6eb-4616-8ec0-0d7aaf14096c",
      "metadata": {
        "scrolled": true,
        "id": "e05d9c0b-c6eb-4616-8ec0-0d7aaf14096c"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns',70)\n",
        "\n",
        "X_train.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d87dc0-83b0-44e5-9f2e-1bd180a3090a",
      "metadata": {
        "id": "f0d87dc0-83b0-44e5-9f2e-1bd180a3090a"
      },
      "outputs": [],
      "source": [
        "# Standard Scaler --> -4 to +4\n",
        "# MinMaxScaler (Normalization) --> 0 to 1\n",
        "# Since our 50% of dataset values are already binary (0/1), so we are going to use MinMaxScaler so that out whole dataset would be binary\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
        "# If we are scaling before splitting, we just need to fit transform data once, but if we are doing after scaling, we have to fit_transform X_train and just transform X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f44cb28-f5a3-4e29-8d6b-fd8048d21db9",
      "metadata": {
        "id": "3f44cb28-f5a3-4e29-8d6b-fd8048d21db9"
      },
      "source": [
        "### Task 30: Prepare ML Model, make predictions and perform evaluation\n",
        "- Logistic Regression\n",
        "- RandomForest\n",
        "- SVM/NaiveBayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93a432e-c720-455c-a901-44004baf5157",
      "metadata": {
        "id": "c93a432e-c720-455c-a901-44004baf5157"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a33fc3-0392-43a8-a774-5f1c7215dc0b",
      "metadata": {
        "id": "94a33fc3-0392-43a8-a774-5f1c7215dc0b"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ddedd6-af7b-410b-9310-9b7c955a72c6",
      "metadata": {
        "id": "f5ddedd6-af7b-410b-9310-9b7c955a72c6"
      },
      "outputs": [],
      "source": [
        "ypred_lr = lr_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1df1f5e-7e78-40ef-a4ba-a0ebe4230627",
      "metadata": {
        "id": "a1df1f5e-7e78-40ef-a4ba-a0ebe4230627"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3234a3af-6e85-43d0-b808-618ceb6bafe5",
      "metadata": {
        "id": "3234a3af-6e85-43d0-b808-618ceb6bafe5"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, ypred_lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697e4828-ef92-4e44-9666-6611bd925c20",
      "metadata": {
        "id": "697e4828-ef92-4e44-9666-6611bd925c20"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a772e9-2950-46da-a27b-e398bf8da6e2",
      "metadata": {
        "id": "74a772e9-2950-46da-a27b-e398bf8da6e2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc_model = RandomForestClassifier(n_estimators=500)\n",
        "rfc_model.fit(X_train, y_train)\n",
        "ypred_rfc = rfc_model.predict(X_test)\n",
        "print(classification_report(y_test, ypred_rfc))\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.50      0.51      0.50     20143\n",
        "#            1       0.49      0.49      0.49     19857\n",
        "\n",
        "#     accuracy                           0.50     40000\n",
        "#    macro avg       0.50      0.50      0.50     40000\n",
        "# weighted avg       0.50      0.50      0.50     40000\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58efc07d-5ebc-426a-9abc-5a915ac06d37",
      "metadata": {
        "id": "58efc07d-5ebc-426a-9abc-5a915ac06d37"
      },
      "outputs": [],
      "source": [
        "feature_imp = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': rfc_model.feature_importances_\n",
        "}).sort_values(by='importance', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "debe7816-9f24-4737-a7b4-882c72aec1b9",
      "metadata": {
        "collapsed": true,
        "id": "debe7816-9f24-4737-a7b4-882c72aec1b9"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 70)\n",
        "feature_imp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b878de-2593-47f1-a494-026293909699",
      "metadata": {
        "id": "69b878de-2593-47f1-a494-026293909699"
      },
      "outputs": [],
      "source": [
        "xtrain_imp_features = X_train[['monthly_charges', 'total_charges', 'tenure_months', 'age', 'minute', 'second', 'hour', 'date', 'week_of_year', 'day_of_week', 'month']]\n",
        "xtest_imp_features = X_test[['monthly_charges', 'total_charges', 'tenure_months', 'age', 'minute', 'second', 'hour', 'date', 'week_of_year', 'day_of_week', 'month']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_model.fit(xtrain_imp_features, y_train)\n",
        "ypred_rfc = rfc_model.predict(xtest_imp_features)\n",
        "print(classification_report(y_test, ypred_rfc))"
      ],
      "metadata": {
        "id": "kY-eXYVCg1dF"
      },
      "id": "kY-eXYVCg1dF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7ef0888b-0924-4acf-b5b0-cd4b717f4653",
      "metadata": {
        "id": "7ef0888b-0924-4acf-b5b0-cd4b717f4653"
      },
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2defa6-6b6f-4c0a-b53b-3d675f7fdc8d",
      "metadata": {
        "id": "fa2defa6-6b6f-4c0a-b53b-3d675f7fdc8d"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "svm_model = svm.SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "ypred_svm = svm_model.predict(X_test)\n",
        "print(classification_report(y_test, ypred_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b4c5e4-f9c2-4c3a-8f05-602c1373dff7",
      "metadata": {
        "id": "59b4c5e4-f9c2-4c3a-8f05-602c1373dff7"
      },
      "source": [
        "#### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a68f6053-71c4-4704-b74e-c4565e5d9601",
      "metadata": {
        "id": "a68f6053-71c4-4704-b74e-c4565e5d9601"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "ypred_nb = nb.predict(X_test)\n",
        "print(classification_report(y_test, ypred_nb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576fa175-06a0-45ea-b2e4-fd445ba6529f",
      "metadata": {
        "id": "576fa175-06a0-45ea-b2e4-fd445ba6529f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}